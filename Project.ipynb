{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maheshkulkarni123OS/OperatingSystemByMaheshKulkarni/blob/master/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9dWf2Yc7fuN",
        "colab_type": "code",
        "outputId": "2dcf3444-fbf7-4886-92aa-13a6de9a0f7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc --version\n",
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n",
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-sn6qxew3\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-sn6qxew3\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4307 sha256=35ff992d00c8a60ed3da743484f3e7827b88bee9ce6446b735c13f7cfd931875\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-r2gjjln8/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrlmys3E8Tb-",
        "colab_type": "code",
        "outputId": "49713317-a085-4230-ab2e-c50e3e1c4d9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "%%cu\n",
        "#ifdef __INTELLISENSE__\n",
        "void __syncthreads();\n",
        "#endif\n",
        "\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <iostream>\n",
        "#include <chrono>\n",
        "\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "#define WSIZE 32\n",
        "#define LOOPS 100\n",
        "#define UPPER_BIT 10\n",
        "#define LOWER_BIT 0\n",
        "\n",
        "__device__ unsigned int ddata[WSIZE];\n",
        "__device__ int ddata_s[WSIZE];\n",
        "\n",
        "template <typename T, unsigned S>\n",
        "inline unsigned arraysize(const T(&v)[S])\n",
        "{\n",
        "\treturn S;\n",
        "}\n",
        "\n",
        "template<typename T>\n",
        "void printArray(T &arr)\n",
        "{\n",
        "\tfor (int i = 0; i < arraysize(arr); ++i)\n",
        "\t{\n",
        "\t\tcout << \"Array[\" << i << \"]: \" << *(arr + i) << endl;\n",
        "\t}\n",
        "}\n",
        "\n",
        "template<typename T>\n",
        "void fillArray(T &arr)\n",
        "{\n",
        "\tsrand(time(NULL));\n",
        "\tfor (int i = 0; i < arraysize(arr); ++i)\n",
        "\t{\n",
        "\t\tarr[i] = rand() % 1024;\n",
        "\t}\n",
        "}\n",
        "\n",
        "void print(int arr[], int n)\n",
        "{\n",
        "\tfor (int i = 0; i < n; i++)\n",
        "\t{\n",
        "\t\tcout << arr[i] << \" \";\n",
        "\t}\n",
        "\tcout << endl;\n",
        "}\n",
        "\n",
        "template<typename T>\n",
        "T findMax(T &arr)\n",
        "{\n",
        "\tT max = 0;\n",
        "\tfor (int i = 0; i < arraysize(arr); ++i)\n",
        "\t{\n",
        "\t\tif (arr[i] > max)\n",
        "\t\t{\n",
        "\t\t\tmax = arr[i];\n",
        "\t\t}\n",
        "\t}\n",
        "\treturn max;\n",
        "}\n",
        "\n",
        "__device__ int getMax(int arr[], int n)\n",
        "{\n",
        "\tint mx = arr[0];\n",
        "\tfor (int i = 1; i < n; i++)\n",
        "\t\tif (arr[i] > mx)\n",
        "\t\t\tmx = arr[i];\n",
        "\treturn mx;\n",
        "}\n",
        "\n",
        "__device__ void countSort(int arr[], int n, int exp)\n",
        "{\n",
        "\tint output[1024]; // Output array\n",
        "\tint i, count[10] = { 0 };\n",
        "\n",
        "\t// Store count of occurrences in count[]\n",
        "\tfor (i = 0; i < n; i++)\n",
        "\t\tcount[(arr[i] / exp) % 10]++;\n",
        "\n",
        "\t// Change count[i] so that count[i] now contains actual\n",
        "\t// position of this digit in output[]\n",
        "\tfor (i = 1; i < 10; i++)\n",
        "\t{\n",
        "\t\tcount[i] += count[i - 1];\n",
        "\t}\n",
        "\n",
        "\t// Build the output array\n",
        "\tfor (i = n - 1; i >= 0; i--)\n",
        "\t{\n",
        "\t\toutput[count[(arr[i] / exp) % 10] - 1] = arr[i];\n",
        "\t\tcount[(arr[i] / exp) % 10]--;\n",
        "\t}\n",
        "\n",
        "\t// Copy the output array to arr[], so that arr[] now\n",
        "\t// contains sorted numbers according to current digit\n",
        "\tfor (i = 0; i < n; i++)\n",
        "\t\tarr[i] = output[i];\n",
        "}\n",
        "\n",
        "__device__ void radixsort(int arr[], int n)\n",
        "{\n",
        "\t// Find the maximum number to know number of digits\n",
        "\tint m = getMax(arr, n);\n",
        "\n",
        "\t// Do counting sort for every digit. Note that instead\n",
        "\t// of passing digit number, exp is passed. exp is 10^i\n",
        "\t// where i is current digit number\n",
        "\tfor (int exp = 1; m / exp > 0; exp *= 10)\n",
        "\t\tcountSort(arr, n, exp);\n",
        "}\n",
        "\n",
        "__global__ void serialRadix()\n",
        "{\n",
        "\tradixsort(ddata_s, WSIZE);\n",
        "\t__syncthreads();\n",
        "}\n",
        "\n",
        "__global__ void parallelRadix()\n",
        "{\n",
        "\t// This data in shared memory\n",
        "\t__shared__ volatile unsigned int sdata[WSIZE * 2];\n",
        "\n",
        "\t// Load from global into shared variable\n",
        "\tsdata[threadIdx.x] = ddata[threadIdx.x];\n",
        "\n",
        "\tunsigned int bitmask = 1 << LOWER_BIT;\n",
        "\tunsigned int offset = 0;\n",
        "\t// -1, -2, -4, -8, -16, -32, -64, -128, -256,...\n",
        "\tunsigned int thrmask = 0xFFFFFFFFU << threadIdx.x;\n",
        "\tunsigned int mypos;\n",
        "\n",
        "\t// For each LSB to MSB\n",
        "\tfor (int i = LOWER_BIT; i <= UPPER_BIT; i++)\n",
        "\t{\n",
        "\t\tunsigned int mydata = sdata[((WSIZE - 1) - threadIdx.x) + offset];\n",
        "\t\tunsigned int mybit = mydata&bitmask;\n",
        "\t\t// Get population of ones and zeroes\n",
        "\t\tunsigned int ones = __ballot(mybit);\n",
        "\t\tunsigned int zeroes = ~ones;\n",
        "\t\t// Switch ping-pong buffers\n",
        "\t\toffset ^= WSIZE;\n",
        "\n",
        "\t\t// Do zeroes, then ones\n",
        "\t\tif (!mybit)\n",
        "\t\t{\n",
        "\t\t\tmypos = __popc(zeroes&thrmask);\n",
        "\t\t}\n",
        "\t\telse  {      // Threads with a one bit\n",
        "\t\t\t// Get my position in ping-pong buffer\n",
        "\t\t\tmypos = __popc(zeroes) + __popc(ones&thrmask);\n",
        "\t\t}\n",
        "\n",
        "\t\t// Move to buffer  (or use shfl for cc 3.0)\n",
        "\t\tsdata[mypos - 1 + offset] = mydata;\n",
        "\t\t// Repeat for next bit\n",
        "\t\tbitmask <<= 1;\n",
        "\t}\n",
        "\t// Put results to global\n",
        "\tddata[threadIdx.x] = sdata[threadIdx.x + offset];\n",
        "}\n",
        "\n",
        "int main() {\n",
        "\n",
        "\t/* Parallel Radix Sort */\n",
        "\n",
        "\tunsigned int hdata[WSIZE];\n",
        "\tfloat totalTime = 0;\n",
        "\n",
        "\tfor (int lcount = 0; lcount < LOOPS; lcount++)\n",
        "\t{\n",
        "\t\tsrand(time(NULL));\n",
        "\t\t\n",
        "\t\tunsigned int range = 1U << UPPER_BIT;\n",
        "\n",
        "\t\t// Fill array with random elements\n",
        "\t\t// Range = 1024\n",
        "\t\tfor (int i = 0; i < WSIZE; i++)\n",
        "\t\t{\n",
        "\t\t\thdata[i] = i;\n",
        "\t\t}\n",
        "\n",
        "\t\t\n",
        "\t\tcudaMemcpyToSymbol(ddata, hdata, WSIZE * sizeof(unsigned int));\n",
        "\n",
        "\t\t\n",
        "\t\thigh_resolution_clock::time_point t1 = high_resolution_clock::now();\n",
        "\t\tparallelRadix <<< 1, WSIZE >>>();\n",
        "\t\t// Make kernel function synchronous\n",
        "\t\tcudaDeviceSynchronize();\n",
        "\t\t// Execution time measurement, that point stops the clock\n",
        "\t\thigh_resolution_clock::time_point t2 = high_resolution_clock::now();\n",
        "\n",
        "\t\t// Execution time measurement, that is the result\n",
        "\t\tauto duration = duration_cast<milliseconds>(t2 - t1).count();\n",
        "\n",
        "\t\t// Summination of each loops' execution time\n",
        "\t\ttotalTime += (float)duration / 1000.00;\n",
        "\n",
        "\t\t// Copy data from device to host\n",
        "\t\tcudaMemcpyFromSymbol(hdata, ddata, WSIZE * sizeof(unsigned int));\n",
        "\t}\n",
        "\n",
        "\tprintf(\"Parallel Radix Sort:\\n\");\n",
        "\tprintf(\"Array size = %d\\n\", WSIZE * LOOPS);\n",
        "\tprintf(\"Time elapsed = %fseconds\\n\", totalTime);\n",
        "\n",
        "\t/* Serial Radix Sort */\n",
        "\n",
        "\tunsigned int hdata_s[WSIZE];\n",
        "\ttotalTime = 0;\n",
        "\n",
        "\tfor (int lcount = 0; lcount < LOOPS; lcount++)\n",
        "\t{\n",
        "\t\tsrand(time(NULL));\n",
        "\t\t// Array elements have value in range of 1024\n",
        "\t\tunsigned int range = 1U << UPPER_BIT;\n",
        "\n",
        "\t\t// Fill array with random elements\n",
        "\t\t// Range = 1024\n",
        "\t\tfor (int i = 0; i < WSIZE; i++)\n",
        "\t\t{\n",
        "\t\t\thdata_s[i] = i;\n",
        "\t\t}\n",
        "\n",
        "\t\t// Copy data from host to device\n",
        "\t\tcudaMemcpyToSymbol(ddata_s, hdata_s, WSIZE * sizeof(unsigned int));\n",
        "\n",
        "\t\t// Execution time measurement, that point starts the clock\n",
        "\t\thigh_resolution_clock::time_point t1 = high_resolution_clock::now();\n",
        "\t\tserialRadix <<< 1, 1 >>>();\n",
        "\t\t// Make kernel function synchronous\n",
        "\t\tcudaDeviceSynchronize();\n",
        "\t\t// Execution time measurement, that point stops the clock\n",
        "\t\thigh_resolution_clock::time_point t2 = high_resolution_clock::now();\n",
        "\n",
        "\t\t// Execution time measurement, that is the result\n",
        "\t\tauto duration = duration_cast<milliseconds>(t2 - t1).count();\n",
        "\n",
        "\t\t// Summination of each loops' execution time\n",
        "\t\ttotalTime += (float)duration / 1000.00;\n",
        "\n",
        "\t\t// Copy data from device to host\n",
        "\t\tcudaMemcpyFromSymbol(hdata_s, ddata_s, WSIZE * sizeof(unsigned int));\n",
        "\t}\n",
        "\n",
        "\tprintf(\"\\nSerial Radix Sort:\\n\");\n",
        "\tprintf(\"Array size = %d\\n\", WSIZE * LOOPS);\n",
        "\tprintf(\"Time elapsed = %fseconds\\n\\n\", totalTime);\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parallel Radix Sort:\n",
            "Array size = 3200\n",
            "Time elapsed = 0.000000seconds\n",
            "\n",
            "Serial Radix Sort:\n",
            "Array size = 3200\n",
            "Time elapsed = 0.006000seconds\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOfHaFDW98_D",
        "colab_type": "code",
        "outputId": "4c313494-343b-4c55-df92-c02b2cc044f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "\n",
        "%%cu\n",
        "#include<stdio.h>\n",
        "__global__ void kernel(void){\n",
        "\t\tprintf(\"Hello World\");\n",
        "\n",
        "}\n",
        "int main(void)\n",
        "{\n",
        "\tkernel<<< 1,1 >>>();\n",
        "\tprintf(\"Hello World!\\n\");\n",
        "\treturn 0;\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello World!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWYj4ZiUJJ48",
        "colab_type": "code",
        "outputId": "0f1b65f8-0cff-49ae-cb6a-780d8a5570a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "#Assignment 5\n",
        "%%cu\n",
        "#include <pthread.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "const int N = 1 << 20;\n",
        "\n",
        "__global__ void kernel(float *x, int n)\n",
        "{\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    printf(\"Tid = %d\",tid);\n",
        "   \n",
        "}\n",
        "\n",
        "void *smaa(void *dummy)\n",
        "{\n",
        "    float *data;\n",
        "    static int cnt=0;\n",
        "    cudaMalloc(&data, N * sizeof(float));\n",
        "    printf(\"\\nLaunch Kernel %d\",cnt);\n",
        "    cnt++;\n",
        "    kernel<<<1, 32>>>(data, N);\n",
        "\n",
        "    cudaStreamSynchronize(0);\n",
        "\n",
        "    return NULL;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    const int num_threads = 8;\n",
        "\n",
        "    pthread_t threads[num_threads];\n",
        "\n",
        "    for (int i = 0; i < num_threads; i++) {\n",
        "        if (pthread_create(&threads[i], NULL, smaa, 0)) {\n",
        "            fprintf(stderr, \"Error creating threadn\");\n",
        "            return 1;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < num_threads; i++) {\n",
        "        if(pthread_join(threads[i], NULL)) {\n",
        "            fprintf(stderr, \"Error joining threadn\");\n",
        "            return 2;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cudaDeviceReset();\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Launch Kernel 0\n",
            "Launch Kernel 1Tid = 0Tid = 1Tid = 2Tid = 3Tid = 4Tid = 5Tid = 6Tid = 7Tid = 8Tid = 9Tid = 10Tid = 11Tid = 12Tid = 13Tid = 14Tid = 15Tid = 16Tid = 17Tid = 18Tid = 19Tid = 20Tid = 21Tid = 22Tid = 23Tid = 24Tid = 25Tid = 26Tid = 27Tid = 28Tid = 29Tid = 30Tid = 31\n",
            "Launch Kernel 2Tid = 0Tid = 1Tid = 2Tid = 3Tid = 4Tid = 5Tid = 6Tid = 7Tid = 8Tid = 9Tid = 10Tid = 11Tid = 12Tid = 13Tid = 14Tid = 15Tid = 16Tid = 17Tid = 18Tid = 19Tid = 20Tid = 21Tid = 22Tid = 23Tid = 24Tid = 25Tid = 26Tid = 27Tid = 28Tid = 29Tid = 30Tid = 31\n",
            "Launch Kernel 3Tid = 0Tid = 1Tid = 2Tid = 3Tid = 4Tid = 5Tid = 6Tid = 7Tid = 8Tid = 9Tid = 10Tid = 11Tid = 12Tid = 13Tid = 14Tid = 15Tid = 16Tid = 17Tid = 18Tid = 19Tid = 20Tid = 21Tid = 22Tid = 23Tid = 24Tid = 25Tid = 26Tid = 27Tid = 28Tid = 29Tid = 30Tid = 31\n",
            "Launch Kernel 3Tid = 0Tid = 1Tid = 2Tid = 3Tid = 4Tid = 5Tid = 6Tid = 7Tid = 8Tid = 9Tid = 10Tid = 11Tid = 12Tid = 13Tid = 14Tid = 15Tid = 16Tid = 17Tid = 18Tid = 19Tid = 20Tid = 21Tid = 22Tid = 23Tid = 24Tid = 25Tid = 26Tid = 27Tid = 28Tid = 29Tid = 30Tid = 31\n",
            "Launch Kernel 3Tid = 0Tid = 1Tid = 2Tid = 3Tid = 4Tid = 5Tid = 6Tid = 7Tid = 8Tid = 9Tid = 10Tid = 11Tid = 12Tid = 13Tid = 14Tid = 15Tid = 16Tid = 17Tid = 18Tid = 19Tid = 20Tid = 21Tid = 22Tid = 23Tid = 24Tid = 25Tid = 26Tid = 27Tid = 28Tid = 29Tid = 30Tid = 31\n",
            "Launch Kernel 3Tid = 0Tid = 1Tid = 2Tid = 3Tid = 4Tid = 5Tid = 6Tid = 7Tid = 8Tid = 9Tid = 10Tid = 11Tid = 12Tid = 13Tid = 14Tid = 15Tid = 16Tid = 17Tid = 18Tid = 19Tid = 20Tid = 21Tid = 22Tid = 23Tid = 24Tid = 25Tid = 26Tid = 27Tid = 28Tid = 29Tid = 30Tid = 31\n",
            "Launch Kernel 3Tid = 0Tid = 1Tid = 2Tid = 3Tid = 4Tid = 5Tid = 6Tid = 7Tid = 8Tid = 9Tid = 10Tid = 11Tid = 12Tid = 13Tid = 14Tid = 15Tid = 16Tid = 17Tid = 18Tid = 19Tid = 20Tid = 21Tid = 22Tid = 23Tid = 24Tid = 25Tid = 26Tid = 27Tid = 28Tid = 29Tid = 30Tid = 31Tid = 0Tid = 1Tid = 2Tid = 3Tid = 4Tid = 5Tid = 6Tid = 7Tid = 8Tid = 9Tid = 10Tid = 11Tid = 12Tid = 13Tid = 14Tid = 15Tid = 16Tid = 17Tid = 18Tid = 19Tid = 20Tid = 21Tid = 22Tid = 23Tid = 24Tid = 25Tid = 26Tid = 27Tid = 28Tid = 29Tid = 30Tid = 31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5xTId_TLN_C",
        "colab_type": "code",
        "outputId": "a47172d4-dfcf-4372-d1cb-a13aecc0703f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Assignment Number 2\n",
        "#1]Vector addition using multiple block single thread\n",
        "%%cu\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "#include<time.h>\n",
        "__global__ void add(int *a, int *b, int *c)\n",
        "{\n",
        "  c[blockIdx.x]=a[blockIdx.x]+b[blockIdx.x];\n",
        "}\n",
        "#define N 512\n",
        "int main(void)\n",
        "{\n",
        "  int *a, *b,*c;\n",
        "  int i;\n",
        "  int *dev_a, *dev_b, *dev_c;\n",
        "  int size=N*sizeof(int);\n",
        "  cudaMalloc((void**)&dev_a,size);\n",
        "  cudaMalloc((void**)&dev_b,size);\n",
        "  cudaMalloc((void**)&dev_c,size);\n",
        "  a=(int*)malloc(size);\n",
        "  b=(int*)malloc(size);\n",
        "  c=(int*)malloc(size);\n",
        "  \n",
        "\n",
        "  for(i=0;i<N;i++)\n",
        "  {\n",
        "    a[i]=rand()%10;\n",
        "    b[i]=rand()%10;\n",
        "  }\n",
        "\n",
        "  cudaMemcpy(dev_a,a,size,cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(dev_b,b,size,cudaMemcpyHostToDevice);\n",
        "  add<<<N,1>>>(dev_a,dev_b,dev_c);\n",
        "  cudaMemcpy(c,dev_c,size,cudaMemcpyDeviceToHost);\n",
        "  for(i=0;i<N;i++)\n",
        "    printf(\"%d \",c[i]);\n",
        "  free(a);\n",
        "  free(b);\n",
        "  free(c);\n",
        "  cudaFree(dev_a);\n",
        "  cudaFree(dev_b);\n",
        "  cudaFree(dev_c);\n",
        "return 0;\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9 12 8 8 10 9 9 9 6 8 9 16 2 5 12 11 10 16 9 3 12 10 11 12 5 9 1 9 2 7 10 11 11 15 10 9 7 11 8 3 15 14 12 4 13 2 14 11 12 13 5 12 8 9 9 8 1 7 14 13 9 8 8 6 16 13 10 11 9 13 9 11 12 8 13 9 9 13 7 6 17 12 1 3 8 8 13 4 12 15 3 15 12 5 13 16 8 16 4 17 10 6 11 6 12 16 16 13 7 3 9 7 4 14 10 11 6 14 9 9 10 12 14 2 1 6 1 13 6 10 7 14 10 11 6 6 11 9 9 4 1 7 3 10 15 12 11 5 12 4 4 8 13 11 17 9 10 18 6 9 7 5 6 18 5 10 7 13 7 12 4 11 7 4 4 2 10 11 2 11 8 4 10 15 7 8 18 9 5 2 13 8 8 7 5 15 6 8 5 10 17 10 16 14 11 6 11 4 13 17 6 15 9 6 10 5 11 8 13 16 6 5 4 15 8 8 11 12 11 11 1 15 10 9 11 12 13 13 14 8 7 9 18 15 7 7 9 2 10 8 3 7 4 10 5 13 11 6 11 12 13 8 6 10 8 17 6 7 1 4 14 8 9 2 11 9 13 6 3 15 14 4 13 6 16 14 15 8 11 10 12 15 11 10 9 16 13 7 8 11 9 16 12 6 10 3 2 3 12 8 5 8 2 7 4 13 7 8 10 10 8 9 2 11 12 10 9 16 11 10 7 6 13 12 5 9 6 9 17 11 7 4 7 7 9 9 7 7 16 7 17 2 11 11 8 6 11 6 8 13 8 8 4 2 8 12 5 5 11 11 14 9 13 10 7 5 11 8 3 8 10 4 9 4 5 8 10 0 7 11 11 7 6 7 14 8 11 5 9 7 4 9 8 12 7 17 4 6 16 14 7 12 12 7 13 3 18 6 10 18 8 12 5 17 13 10 8 4 6 9 4 1 10 5 4 3 3 12 11 6 3 9 5 11 15 14 9 9 11 16 4 7 3 3 11 5 3 10 6 7 17 5 6 11 15 3 5 13 5 10 10 3 14 13 10 10 8 17 8 10 6 11 8 16 13 9 6 16 9 11 14 12 11 15 7 9 5 10 9 3 7 13 10 12 5 4 7 2 4 16 7 11 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9A_gfjbLTGj",
        "colab_type": "code",
        "outputId": "d43a7dbb-e2d7-4b40-e061-f3049315946e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#2]Vector Addition using single thread single block \n",
        "\n",
        "%%cu\n",
        "#include<stdio.h>\n",
        "__global__ void add(int *a, int *b, int *c)\n",
        "{\n",
        "  *c= *a + *b;\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "int a,b,c;\n",
        "int *dev_a,*dev_b,*dev_c;\n",
        "int size=sizeof(int);\n",
        "  cudaMalloc((void **)&dev_a,size);\n",
        "  cudaMalloc((void **)&dev_b,size);\n",
        "  cudaMalloc((void **)&dev_c,size);\n",
        "  a=2;\n",
        "  b=7;\n",
        "  cudaMemcpy(dev_a,&a,size,cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(dev_b,&b,size,cudaMemcpyHostToDevice);\n",
        "  add<<< 1,1 >>>(dev_a,dev_b,dev_c);\n",
        "  cudaMemcpy(&c,dev_c,size,cudaMemcpyDeviceToHost);\n",
        "  printf(\"addition = %d\",c);\n",
        "  cudaFree(dev_a);\n",
        "  cudaFree(dev_b);\n",
        "  cudaFree(dev_c);\n",
        "\n",
        "return 0;\n",
        "\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "addition = 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFvg9t3tLfvi",
        "colab_type": "code",
        "outputId": "d59f0f09-3462-4389-a7a4-6aab4caae213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#]Vector Addition using multiple thread single block\n",
        "%%cu\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "#include<time.h>\n",
        "#define N 512\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c ) {\n",
        "c[threadIdx.x] = a[threadIdx.x] + b[ threadIdx.x];\n",
        "}\n",
        "\n",
        "\n",
        "int main( void ) {\n",
        "\n",
        "int *a, *b, *c;\n",
        "int *dev_a, *dev_b, *dev_c;\n",
        "int size = N * sizeof( int ); \n",
        "cudaMalloc( (void**)&dev_a, size );\n",
        "cudaMalloc( (void**)&dev_b, size );\n",
        "cudaMalloc( (void**)&dev_c, size );\n",
        "a = (int*)malloc( size );\n",
        "b = (int*)malloc( size );\n",
        "c = (int*)malloc( size );\n",
        "\n",
        "for(int i=0;i<N;i++)\n",
        "{\n",
        "  a[i]=rand()%10;\n",
        "  b[i]=rand()%10;\n",
        "}\n",
        "cudaMemcpy(dev_a, a, size, cudaMemcpyHostToDevice );\n",
        "cudaMemcpy(dev_b, b, size, cudaMemcpyHostToDevice );\n",
        "add<<< 1,N >>>( dev_a, dev_b, dev_c );\n",
        "cudaMemcpy( c, dev_c, size, cudaMemcpyDeviceToHost );\n",
        "for(int i=0;i<N;i++)\n",
        "printf(\"%d \",c[i]);\n",
        "free( a ); \n",
        "free( b );\n",
        "free( c );\n",
        "cudaFree(dev_a );\n",
        "cudaFree(dev_b );\n",
        "cudaFree(dev_c );\n",
        "return 0;\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9 12 8 8 10 9 9 9 6 8 9 16 2 5 12 11 10 16 9 3 12 10 11 12 5 9 1 9 2 7 10 11 11 15 10 9 7 11 8 3 15 14 12 4 13 2 14 11 12 13 5 12 8 9 9 8 1 7 14 13 9 8 8 6 16 13 10 11 9 13 9 11 12 8 13 9 9 13 7 6 17 12 1 3 8 8 13 4 12 15 3 15 12 5 13 16 8 16 4 17 10 6 11 6 12 16 16 13 7 3 9 7 4 14 10 11 6 14 9 9 10 12 14 2 1 6 1 13 6 10 7 14 10 11 6 6 11 9 9 4 1 7 3 10 15 12 11 5 12 4 4 8 13 11 17 9 10 18 6 9 7 5 6 18 5 10 7 13 7 12 4 11 7 4 4 2 10 11 2 11 8 4 10 15 7 8 18 9 5 2 13 8 8 7 5 15 6 8 5 10 17 10 16 14 11 6 11 4 13 17 6 15 9 6 10 5 11 8 13 16 6 5 4 15 8 8 11 12 11 11 1 15 10 9 11 12 13 13 14 8 7 9 18 15 7 7 9 2 10 8 3 7 4 10 5 13 11 6 11 12 13 8 6 10 8 17 6 7 1 4 14 8 9 2 11 9 13 6 3 15 14 4 13 6 16 14 15 8 11 10 12 15 11 10 9 16 13 7 8 11 9 16 12 6 10 3 2 3 12 8 5 8 2 7 4 13 7 8 10 10 8 9 2 11 12 10 9 16 11 10 7 6 13 12 5 9 6 9 17 11 7 4 7 7 9 9 7 7 16 7 17 2 11 11 8 6 11 6 8 13 8 8 4 2 8 12 5 5 11 11 14 9 13 10 7 5 11 8 3 8 10 4 9 4 5 8 10 0 7 11 11 7 6 7 14 8 11 5 9 7 4 9 8 12 7 17 4 6 16 14 7 12 12 7 13 3 18 6 10 18 8 12 5 17 13 10 8 4 6 9 4 1 10 5 4 3 3 12 11 6 3 9 5 11 15 14 9 9 11 16 4 7 3 3 11 5 3 10 6 7 17 5 6 11 15 3 5 13 5 10 10 3 14 13 10 10 8 17 8 10 6 11 8 16 13 9 6 16 9 11 14 12 11 15 7 9 5 10 9 3 7 13 10 12 5 4 7 2 4 16 7 11 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5tybxmkLil1",
        "colab_type": "code",
        "outputId": "9745e2f3-3868-49d5-9c7b-71210d824543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#4]Vector addition using multiple threads multiple blocks\n",
        "%%cu\n",
        "#include<stdio.h>\n",
        "#include<time.h>\n",
        "#define N (64*64)\n",
        "#define THREADS_PER_BLOCK 512\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c ) {\n",
        "int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "c[index] = a[index] + b[index];\n",
        "}\n",
        "\n",
        "int main( void ) {\n",
        "int *a, *b, *c;\n",
        "int *dev_a, *dev_b, *dev_c;\n",
        "int size = N * sizeof( int ); \n",
        "cudaMalloc( (void**)&dev_a, size );\n",
        "cudaMalloc( (void**)&dev_b, size );\n",
        "cudaMalloc( (void**)&dev_c, size );\n",
        "a = (int*)malloc( size );\n",
        "b = (int*)malloc( size );\n",
        "c = (int*)malloc( size );\n",
        "\n",
        "for(int i=0;i<N;i++)\n",
        "{\n",
        "a[i]=rand()%10;\n",
        "b[i]=rand()%10;\n",
        "}\n",
        "cudaMemcpy(dev_a, a, size, cudaMemcpyHostToDevice );\n",
        "cudaMemcpy(dev_b, b, size, cudaMemcpyHostToDevice );\n",
        "add<<< N/THREADS_PER_BLOCK, THREADS_PER_BLOCK >>>( dev_a, dev_b, dev_c );\n",
        "cudaMemcpy( c, dev_c, size, cudaMemcpyDeviceToHost );\n",
        "\n",
        "for(int i=0;i<N;i++)\n",
        "{\n",
        "  printf(\"%d \",c[i]);\n",
        "}\n",
        "\n",
        "free( a ); free( b ); free( c );\n",
        "cudaFree(dev_a );\n",
        "cudaFree(dev_b );\n",
        "cudaFree(dev_c );\n",
        "return 0;\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9 12 8 8 10 9 9 9 6 8 9 16 2 5 12 11 10 16 9 3 12 10 11 12 5 9 1 9 2 7 10 11 11 15 10 9 7 11 8 3 15 14 12 4 13 2 14 11 12 13 5 12 8 9 9 8 1 7 14 13 9 8 8 6 16 13 10 11 9 13 9 11 12 8 13 9 9 13 7 6 17 12 1 3 8 8 13 4 12 15 3 15 12 5 13 16 8 16 4 17 10 6 11 6 12 16 16 13 7 3 9 7 4 14 10 11 6 14 9 9 10 12 14 2 1 6 1 13 6 10 7 14 10 11 6 6 11 9 9 4 1 7 3 10 15 12 11 5 12 4 4 8 13 11 17 9 10 18 6 9 7 5 6 18 5 10 7 13 7 12 4 11 7 4 4 2 10 11 2 11 8 4 10 15 7 8 18 9 5 2 13 8 8 7 5 15 6 8 5 10 17 10 16 14 11 6 11 4 13 17 6 15 9 6 10 5 11 8 13 16 6 5 4 15 8 8 11 12 11 11 1 15 10 9 11 12 13 13 14 8 7 9 18 15 7 7 9 2 10 8 3 7 4 10 5 13 11 6 11 12 13 8 6 10 8 17 6 7 1 4 14 8 9 2 11 9 13 6 3 15 14 4 13 6 16 14 15 8 11 10 12 15 11 10 9 16 13 7 8 11 9 16 12 6 10 3 2 3 12 8 5 8 2 7 4 13 7 8 10 10 8 9 2 11 12 10 9 16 11 10 7 6 13 12 5 9 6 9 17 11 7 4 7 7 9 9 7 7 16 7 17 2 11 11 8 6 11 6 8 13 8 8 4 2 8 12 5 5 11 11 14 9 13 10 7 5 11 8 3 8 10 4 9 4 5 8 10 0 7 11 11 7 6 7 14 8 11 5 9 7 4 9 8 12 7 17 4 6 16 14 7 12 12 7 13 3 18 6 10 18 8 12 5 17 13 10 8 4 6 9 4 1 10 5 4 3 3 12 11 6 3 9 5 11 15 14 9 9 11 16 4 7 3 3 11 5 3 10 6 7 17 5 6 11 15 3 5 13 5 10 10 3 14 13 10 10 8 17 8 10 6 11 8 16 13 9 6 16 9 11 14 12 11 15 7 9 5 10 9 3 7 13 10 12 5 4 7 2 4 16 7 11 5 3 3 14 17 1 13 1 2 6 16 4 3 6 15 13 6 9 10 10 15 6 4 13 9 12 12 14 3 3 18 4 9 13 10 11 14 8 14 3 9 9 7 6 1 3 4 9 6 18 13 17 7 12 6 10 6 7 9 11 10 3 11 14 10 9 10 7 10 14 4 10 3 13 5 18 8 6 9 3 0 9 9 11 15 4 0 5 4 8 8 10 3 14 12 10 11 9 3 6 16 13 10 13 2 11 14 7 5 10 9 6 11 8 13 12 11 11 9 12 13 10 10 9 9 9 9 5 5 16 9 3 7 12 9 15 5 9 10 15 9 12 12 9 11 1 6 8 10 7 9 8 14 14 14 7 6 18 7 3 5 5 10 13 10 6 7 9 9 12 6 11 10 8 11 4 15 5 9 13 5 3 10 13 8 7 15 4 4 7 15 8 6 11 10 9 2 8 9 8 13 8 7 6 5 3 11 6 12 9 9 7 10 1 13 11 7 10 14 5 10 4 15 13 4 11 16 11 12 0 14 10 11 5 14 8 13 10 9 15 9 12 11 7 10 5 6 15 13 7 7 9 15 6 5 5 12 7 17 12 13 10 10 11 6 8 18 11 11 8 14 10 6 8 12 8 13 7 4 6 13 13 10 11 5 8 6 7 13 11 10 6 6 7 10 8 3 8 9 4 12 13 4 6 9 1 12 6 0 6 9 7 7 6 8 9 4 6 9 6 16 9 13 13 3 1 10 13 12 1 5 9 10 7 14 7 9 3 9 18 3 9 18 7 8 8 15 11 3 2 11 10 1 13 12 16 9 15 3 1 9 10 4 6 6 10 13 8 14 12 10 7 8 12 10 2 11 12 10 13 3 4 9 11 11 9 1 1 6 0 2 4 11 17 9 15 6 6 7 12 9 9 3 8 7 14 7 9 17 7 12 5 6 16 6 10 14 2 9 9 14 16 17 17 8 7 2 7 10 0 6 8 0 11 8 5 2 4 0 12 4 2 9 12 8 6 9 9 12 7 10 12 0 10 8 13 10 10 15 13 7 13 9 4 12 11 11 12 8 6 2 10 7 2 10 0 2 12 15 10 15 11 2 12 12 12 10 7 13 15 13 9 7 12 15 13 11 10 15 9 10 7 9 1 10 1 7 7 7 13 8 6 1 7 8 7 5 13 14 4 7 5 10 9 6 6 16 11 17 17 12 9 6 8 15 10 10 8 13 5 5 7 2 16 13 11 8 10 12 13 6 7 5 7 16 13 9 2 5 15 10 11 4 16 16 6 10 8 14 3 5 10 5 11 7 6 10 10 5 13 15 10 10 8 11 11 5 4 11 7 7 10 10 15 13 11 7 0 7 11 5 11 13 7 8 8 6 8 2 4 2 13 15 6 6 10 6 9 5 7 7 4 8 6 15 8 14 12 8 6 10 12 6 4 15 11 3 15 6 9 10 14 13 7 1 14 8 17 15 12 9 5 6 8 8 11 12 7 8 4 8 11 1 8 7 15 10 4 17 8 5 4 14 10 4 11 15 14 8 7 9 7 12 16 7 5 14 7 8 8 2 0 17 9 13 3 8 12 17 8 10 14 9 9 11 14 1 8 17 8 12 12 18 16 10 5 5 13 5 9 6 9 7 13 2 3 10 11 8 10 10 8 11 8 1 10 13 4 4 14 9 9 3 6 6 13 10 9 8 3 17 14 10 2 13 9 11 10 14 8 11 6 2 9 1 7 13 18 15 6 2 9 17 10 13 8 8 14 16 18 5 3 2 6 9 13 12 8 8 10 6 14 18 12 12 8 5 9 10 10 15 14 12 17 10 7 10 9 1 12 12 12 8 10 15 14 7 15 10 15 6 13 3 4 10 2 12 14 14 16 8 16 17 9 14 4 12 6 15 9 9 7 12 10 8 10 5 11 8 12 16 10 10 3 8 7 8 7 10 9 11 14 15 12 15 12 6 7 13 9 6 9 4 9 10 13 6 12 10 9 7 4 11 5 7 14 7 13 15 6 6 4 4 10 7 0 9 4 7 12 1 15 7 8 7 6 10 13 12 7 9 11 4 11 0 4 5 8 15 11 8 12 1 9 13 11 9 13 11 15 6 18 6 10 8 9 12 10 5 14 9 7 13 7 3 11 10 8 14 10 16 5 7 10 7 5 7 3 16 8 1 7 9 13 12 9 13 12 15 16 8 5 8 13 11 10 10 4 10 7 7 7 8 16 13 11 9 2 8 8 4 10 5 5 9 9 5 14 4 16 8 6 11 12 11 13 16 3 5 8 7 6 6 6 17 6 13 14 7 1 0 10 16 7 6 8 7 6 4 12 12 5 6 9 7 2 0 4 9 10 12 18 3 10 16 5 15 11 4 15 14 15 13 17 7 15 13 15 5 10 12 7 16 8 12 15 17 14 9 7 12 6 13 12 14 8 16 12 7 4 10 11 4 13 5 11 1 14 7 2 14 8 10 18 7 3 12 8 1 5 8 12 10 5 7 11 14 9 12 9 7 10 7 11 7 11 10 14 2 7 14 7 10 10 9 5 9 10 9 9 10 10 8 8 11 5 7 10 15 13 15 5 9 8 12 11 14 11 4 18 7 8 4 7 8 7 16 5 4 11 18 6 10 3 9 5 13 6 18 13 9 14 13 14 9 11 14 10 9 13 8 9 14 7 14 15 7 13 15 7 12 11 3 7 17 5 14 7 7 7 9 14 8 6 3 5 9 8 5 9 3 8 16 11 4 3 8 8 9 8 15 8 18 12 8 15 13 17 9 11 6 13 6 0 0 9 8 18 9 7 10 8 10 8 3 3 11 10 15 15 13 9 6 7 5 5 15 7 10 3 5 6 10 11 4 7 15 10 11 8 18 13 15 7 13 11 2 8 12 6 9 13 7 10 6 6 8 7 5 6 1 3 4 4 8 9 9 6 9 6 2 6 18 14 3 7 13 12 9 8 4 11 3 9 3 14 3 5 4 9 14 9 7 10 8 11 8 4 5 12 5 13 9 16 14 8 12 8 7 10 4 3 11 13 17 12 11 1 1 9 12 3 12 9 10 9 12 14 6 13 11 7 6 7 5 7 8 6 13 12 15 14 9 12 11 7 10 10 10 13 12 15 10 6 7 12 9 14 7 10 8 6 9 13 10 11 10 6 14 17 10 10 9 2 16 12 8 11 11 5 14 9 8 0 7 2 13 9 2 4 11 12 11 4 5 10 2 2 3 12 8 1 13 13 10 2 12 8 4 12 14 9 8 4 12 9 6 3 10 11 0 10 16 10 12 11 3 8 6 4 4 9 9 15 13 4 8 9 10 12 1 5 7 7 12 11 9 12 9 11 9 11 4 17 8 6 17 10 4 7 6 9 3 8 13 10 11 5 9 10 8 3 6 17 10 13 9 17 17 13 9 8 13 14 2 10 9 12 9 13 12 4 11 2 0 15 6 9 4 3 14 15 11 4 3 8 15 7 8 3 2 15 9 10 5 7 10 6 12 11 10 10 16 9 12 10 9 11 16 10 9 12 13 10 13 9 6 8 10 14 8 13 5 6 1 14 12 6 10 7 0 16 3 8 9 2 12 5 9 7 6 15 8 1 10 9 13 8 16 5 6 12 12 17 9 5 4 12 8 3 8 11 17 11 12 8 12 16 15 10 15 8 11 12 18 16 15 3 12 11 8 9 7 11 9 4 12 8 6 11 14 8 12 9 8 14 10 11 10 5 5 0 7 9 5 16 6 6 10 10 18 10 17 12 8 13 12 12 6 13 17 6 11 10 7 8 10 13 6 10 0 7 12 11 4 5 12 14 12 10 11 12 6 14 5 12 8 16 4 15 9 9 10 0 4 7 8 12 11 12 10 8 11 18 18 14 11 14 14 10 2 11 14 8 7 9 10 15 8 4 13 10 8 11 12 8 15 11 13 8 16 4 11 13 8 14 14 13 5 2 10 9 10 7 17 8 12 3 8 14 5 15 11 9 9 11 9 8 9 8 9 7 13 4 5 8 2 7 6 5 3 8 13 9 10 6 1 8 13 15 5 9 13 7 9 10 7 1 2 10 7 15 10 10 4 8 5 14 12 13 13 9 17 15 2 17 11 15 13 9 18 3 12 9 9 12 13 9 13 8 9 13 15 6 9 12 6 9 11 11 9 11 15 13 11 8 9 15 5 1 12 9 9 6 2 10 9 9 8 10 6 5 7 11 16 13 12 14 8 9 11 9 8 11 15 11 6 7 10 4 7 1 8 8 9 14 6 6 13 15 7 7 4 4 10 6 11 13 11 13 6 16 12 9 5 11 13 3 15 7 9 9 8 18 8 9 11 1 13 6 1 8 7 12 14 6 9 1 9 5 6 10 12 9 10 10 14 4 5 11 9 8 6 12 12 4 14 13 11 11 3 10 9 6 7 9 12 11 11 12 15 8 5 9 11 7 11 5 11 12 6 3 15 2 15 6 9 8 5 8 16 6 8 13 11 4 13 13 11 2 10 11 13 7 6 4 6 11 8 12 6 12 14 16 14 4 2 9 13 13 0 5 15 10 9 9 6 15 8 9 7 5 6 3 17 12 5 8 10 10 6 13 8 9 7 10 16 10 6 8 10 7 12 11 11 8 11 8 6 14 2 14 7 6 7 1 13 9 11 15 11 9 11 10 10 9 7 16 5 12 17 15 9 3 1 2 12 12 6 8 13 16 9 9 13 14 14 8 11 13 17 11 14 9 3 12 14 5 6 4 5 15 5 8 9 5 10 14 8 10 10 17 5 7 13 9 9 12 14 6 8 10 5 10 5 2 11 4 9 10 9 9 9 5 7 7 11 12 9 6 12 3 14 13 15 13 11 5 10 11 11 9 14 16 11 14 5 6 4 0 9 17 4 7 13 12 6 15 9 12 17 1 12 11 8 13 14 12 13 2 6 10 11 10 2 10 7 11 9 3 9 7 4 10 6 8 10 1 9 3 5 9 0 10 8 9 15 6 13 7 6 9 3 6 13 9 8 9 7 16 17 17 10 6 7 9 10 16 6 10 9 5 8 11 6 14 10 13 3 8 8 11 9 6 17 4 6 17 9 3 4 8 1 9 9 5 7 11 9 14 11 8 5 7 4 13 0 7 4 11 9 15 9 7 12 5 7 11 5 2 8 3 16 13 2 8 8 7 8 13 2 12 6 5 9 8 12 10 6 14 7 8 12 9 7 7 6 14 11 6 16 13 12 9 3 6 9 9 9 17 16 1 12 12 10 11 7 7 11 14 14 9 3 5 4 12 7 16 13 10 11 7 11 3 9 3 12 16 11 11 11 10 3 14 3 10 4 9 10 8 8 9 14 11 3 8 4 9 11 2 7 5 6 5 7 11 1 11 14 17 10 10 11 10 7 11 9 12 9 9 5 5 9 7 7 11 9 10 14 6 5 12 11 17 8 11 12 7 3 15 8 13 13 8 9 15 8 5 10 3 13 8 11 16 17 10 3 6 13 17 7 13 1 0 6 6 0 14 17 2 8 5 2 10 4 15 13 11 13 1 7 6 8 15 5 9 7 8 15 12 5 14 4 7 7 5 4 7 9 14 5 4 6 9 6 4 10 8 3 6 0 10 5 10 7 5 6 10 17 3 8 13 5 7 7 3 7 15 8 10 16 3 4 8 8 11 8 2 6 14 1 8 10 12 4 9 18 8 1 7 5 16 8 9 14 18 9 15 4 14 7 4 10 10 14 7 6 14 8 15 5 11 7 7 1 6 7 8 1 2 17 11 6 13 16 12 11 1 5 3 12 13 10 2 6 12 11 11 6 3 11 10 4 7 12 15 8 4 14 4 10 4 7 9 16 10 12 7 17 14 9 13 3 11 10 8 17 7 3 12 15 13 10 11 9 12 11 14 6 11 9 11 18 6 1 10 9 12 11 9 7 18 10 11 5 10 1 6 6 11 14 13 9 5 6 8 8 11 5 13 14 0 11 10 5 3 17 8 10 12 3 4 5 10 13 11 6 15 7 11 15 6 8 5 5 9 9 9 4 2 13 9 3 11 9 4 2 3 4 5 12 3 14 10 0 13 12 15 15 10 12 5 14 3 11 11 8 9 5 8 6 14 13 10 11 10 11 15 18 11 2 8 13 9 12 9 6 6 11 11 3 12 11 8 15 4 16 12 6 5 13 6 8 12 5 7 10 9 7 15 4 6 9 7 7 0 4 6 5 17 9 3 11 6 8 7 9 10 15 0 4 7 14 7 9 6 6 12 12 14 8 2 8 10 7 10 18 6 7 10 7 0 5 7 7 7 7 7 10 13 5 1 7 14 10 17 8 5 5 8 8 5 11 8 8 15 12 3 5 13 8 6 10 15 8 17 4 11 10 6 11 8 7 9 4 10 9 9 13 2 5 9 9 5 6 2 9 6 9 12 14 9 5 8 4 11 7 7 16 9 9 8 1 11 15 9 14 10 5 10 15 3 6 2 6 10 12 11 16 11 13 17 16 4 12 9 11 6 10 14 11 1 8 11 11 11 3 8 16 10 11 14 8 5 0 5 7 7 14 6 13 6 8 12 11 12 10 10 7 16 2 12 4 11 9 6 15 15 11 12 8 11 7 4 7 7 10 16 8 15 12 8 12 6 7 11 2 9 15 9 11 8 15 7 8 8 13 4 6 13 7 4 7 15 14 1 12 11 8 15 1 11 9 6 3 6 14 8 11 12 6 9 8 18 17 8 5 12 10 7 7 2 11 2 7 7 3 16 11 4 6 15 10 11 14 13 9 9 10 8 11 8 6 3 5 14 9 8 3 6 9 11 11 14 11 4 7 16 11 1 13 13 9 7 11 16 9 14 6 7 9 7 7 10 9 11 2 13 16 12 10 4 11 9 3 9 6 13 16 9 8 6 7 7 15 7 9 15 3 11 7 10 6 3 12 16 4 6 7 11 5 7 13 10 13 9 12 9 10 8 15 10 4 8 13 9 11 7 7 9 17 6 12 4 8 3 14 13 6 5 8 13 7 3 5 8 15 8 12 8 1 13 3 12 6 4 7 12 6 7 14 13 11 16 10 10 15 16 8 12 11 4 3 6 10 7 9 12 11 10 2 0 2 4 11 10 2 12 10 10 7 11 6 11 10 3 7 9 5 5 15 8 5 8 14 9 12 12 14 12 11 16 5 13 15 7 7 13 10 4 8 10 15 8 12 5 2 7 13 5 15 13 10 15 6 18 13 12 6 13 5 14 4 13 2 10 7 10 15 13 8 13 3 13 9 13 4 13 10 5 11 8 5 13 7 11 13 4 15 10 12 8 2 4 11 0 10 9 12 14 11 2 8 3 10 6 8 6 14 4 7 6 9 9 12 8 5 11 17 3 12 14 2 4 8 9 13 7 11 1 5 1 11 11 15 10 8 4 12 4 3 2 9 4 14 7 13 8 9 5 7 9 9 3 8 12 17 15 2 10 12 13 2 5 14 12 5 15 3 15 12 11 7 12 14 12 6 9 11 6 5 6 8 10 9 7 18 8 11 3 17 10 5 8 9 12 15 9 15 11 6 10 5 6 5 9 10 12 5 4 9 7 10 7 4 18 13 11 4 10 5 9 3 9 9 9 3 11 7 5 5 11 8 10 10 13 5 6 13 2 6 6 13 9 14 2 14 4 7 13 12 6 17 12 12 7 11 3 10 5 4 12 11 8 9 11 8 8 0 9 9 5 9 15 8 11 6 9 16 7 8 4 13 17 8 4 10 10 4 6 10 4 6 4 8 13 8 10 7 3 9 10 12 8 8 5 10 11 10 6 5 3 7 3 12 5 1 9 13 10 2 9 2 7 3 8 10 10 15 8 15 17 17 8 13 8 10 5 3 1 11 12 5 15 8 7 7 8 3 9 3 11 11 11 12 11 10 12 10 5 10 11 7 15 6 12 9 16 14 4 8 6 15 10 7 12 5 8 9 4 5 16 17 12 15 6 8 11 10 15 4 10 15 7 10 11 5 7 6 8 13 15 14 4 13 9 10 14 15 12 10 5 9 2 7 7 2 12 15 6 9 5 2 10 17 16 6 17 0 8 6 6 15 2 5 8 11 4 9 8 3 9 15 11 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FTSlO2nMLZH",
        "colab_type": "code",
        "outputId": "e3090c76-86c6-4dce-b370-090b5c3ce1e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Assignment 3\n",
        "#Matrix Multiplication-\n",
        "%%cu\n",
        "#define N 3\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void matrixMult (int *a, int *b, int *c, int width)\n",
        "{\n",
        "int i, sum = 0;\n",
        "int col = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "int row = threadIdx.y + blockDim.y * blockIdx.y;\n",
        "if(col < width && row < width)\n",
        "for (i = 0; i< width; i++)\n",
        "{\n",
        "sum += a[row * width + i] * b[i * width + col];\n",
        "}\n",
        "c[row * width + col] = sum;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "int a[N][N], b[N][N], c[N][N];\n",
        "int *dev_a, *dev_b, *dev_c;\n",
        " int i=1;\n",
        "for (int y = 0; y < N; y++)\n",
        "                          {\n",
        "for (int x = 0; x < N; x++)\n",
        "a[y][x]=i++;\n",
        "                          }\n",
        "i=9;\n",
        "   for (int y = 0; y < N; y++)\n",
        "        {\n",
        "       for (int x = 0; x < N; x++)\n",
        "                b[y][x]=i--;\n",
        "         }\n",
        "\n",
        "int size = N * N * sizeof(int);\n",
        "cudaMalloc((void **) &dev_a, size);\n",
        "cudaMalloc((void **) &dev_b, size);\n",
        "cudaMalloc((void **) &dev_c, size);\n",
        "cudaMemcpy(dev_a, a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(dev_b, b, size, cudaMemcpyHostToDevice);\n",
        "dim3 dimGrid(1, 1);\n",
        "dim3 dimBlock(N, N);\n",
        "matrixMult<<<dimGrid, dimBlock>>>(dev_a, dev_b, dev_c, N);\n",
        "cudaMemcpy(c, dev_c, size, cudaMemcpyDeviceToHost);\n",
        "cudaFree(dev_a);\n",
        "cudaFree(dev_b);\n",
        "cudaFree(dev_c);\n",
        "for (int y = 0; y < N; y++) {\n",
        "for (int x = 0; x < N; x++) {\n",
        "printf(\"%d \\t\", c[y][x]);\n",
        "}\n",
        "printf(\"\\n\");\n",
        "}\n",
        "return 0;\n",
        "\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30 \t24 \t18 \t\n",
            "84 \t69 \t54 \t\n",
            "138 \t114 \t90 \t\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQN8rd9kMTsT",
        "colab_type": "code",
        "outputId": "a495093a-a0fc-4934-cc2d-8f348f6e05b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Assignment 3\n",
        "#Dot product using shared memory-\n",
        "%%cu\n",
        "#include<stdio.h>\n",
        "#define N (2048*2048)\n",
        "#define THREADS_PER_BLOCK 512\n",
        "__global__ void dot( int *a, int *b, int *c ) {\n",
        "__shared__ int temp[THREADS_PER_BLOCK];\n",
        "int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "temp[threadIdx.x] = a[index] * b[index];\n",
        "__syncthreads();\n",
        "if( 0 == threadIdx.x ) {\n",
        "int sum = 0;\n",
        "for( int i = 0; i < THREADS_PER_BLOCK; i++ )\n",
        "sum += temp[i];\n",
        "atomicAdd( c , sum );\n",
        "}\n",
        "}\n",
        "\n",
        "int main( void ) {\n",
        "int *a, *b, *c,i; \n",
        "int *dev_a, *dev_b, *dev_c;\n",
        "int size = N * sizeof( int ); \n",
        "cudaMalloc( (void**)&dev_a, size );\n",
        "cudaMalloc( (void**)&dev_b, size );\n",
        "cudaMalloc( (void**)&dev_c, sizeof( int ) );\n",
        "a = (int *)malloc( size );\n",
        "b = (int *)malloc( size );\n",
        "c = (int *)malloc( sizeof( int ) );\n",
        "int sumtest= 0;\n",
        "for(i=0;i<N;i++)\n",
        "   {\n",
        "       a[i] = rand() % 10;\n",
        "       b[i] = rand() % 10;\n",
        "       sumtest += a[i]*b[i];\n",
        "   }\n",
        "cudaMemcpy( dev_a, a, size, cudaMemcpyHostToDevice );\n",
        "cudaMemcpy( dev_b, b, size, cudaMemcpyHostToDevice );\n",
        "dot<<< N/THREADS_PER_BLOCK, THREADS_PER_BLOCK >>>( dev_a, dev_b, dev_c );\n",
        "cudaMemcpy( c, dev_c, sizeof( int ) , cudaMemcpyDeviceToHost );\n",
        "printf(\" Using CPU:- %d\\n \",sumtest);\n",
        "printf(\"Using GPU:- %d\",*c);\n",
        "free( a );\n",
        "free( b );\n",
        "free( c );\n",
        "cudaFree( dev_a );\n",
        "cudaFree( dev_b );\n",
        "cudaFree( dev_c );\n",
        "return 0;\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Using CPU:- 85019351\n",
            " Using GPU:- 85019351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRhvvfo3NZJR",
        "colab_type": "code",
        "outputId": "3005640f-d714-4cc4-af3f-e23a71c37b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Assignment 4\n",
        "#Cuda Reduction-\n",
        "%%cu\n",
        "#include \"cuda_runtime.h\"\n",
        "#include \"device_launch_parameters.h\"\n",
        "\n",
        "#include <iostream>\n",
        "#include <numeric>\n",
        "using namespace std;\n",
        "\n",
        "__global__ void sum(int* input)\n",
        "{\n",
        "  const int tid = threadIdx.x;\n",
        "\n",
        "  auto step_size = 1;\n",
        "  int number_of_threads = blockDim.x;\n",
        "\n",
        "  while (number_of_threads > 0)\n",
        "  {\n",
        "    if (tid < number_of_threads)\n",
        "    {\n",
        "      const auto fst = tid * step_size * 2;\n",
        "      const auto snd = fst + step_size;\n",
        "      input[fst] += input[snd];\n",
        "    }\n",
        "\n",
        "    step_size <<= 1; \n",
        "    number_of_threads >>= 1;\n",
        "  }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  const auto count = 8;\n",
        "  const int size = count * sizeof(int);\n",
        "  int h[] = {13, 27, 15, 14, 33, 2, 24, 6};\n",
        "\n",
        "  int* d;\n",
        "  \n",
        "  cudaMalloc(&d, size);\n",
        "  cudaMemcpy(d, h, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  sum <<<1, count / 2 >>>(d);\n",
        "\n",
        "  int result;\n",
        "  cudaMemcpy(&result, d, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  cout << \"Sum is \" << result << endl;\n",
        "\n",
        "  getchar();\n",
        "\n",
        "  cudaFree(d);\n",
        "  delete[] h;\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sum is 134\n",
            "src/tcmalloc.cc:283] Attempt to free invalid pointer 0x7ffce3aac940 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKzEwq7ENee6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}